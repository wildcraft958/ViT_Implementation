{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"tpNymVHy9l1c","outputId":"332b5881-fad0-45c4-b735-6258306d46cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initialized model\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Loaded data\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████| 1/1 [00:11<00:00, 11.78s/it]\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, Subset\n","from tqdm import tqdm\n","\n","class PatchEmbedding(nn.Module):\n","    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=256):\n","        super().__init__()\n","        # CIFAR image size is 32x32, we'll use 4x4 patches\n","        self.patch_size = patch_size\n","        self.n_patches = (img_size // patch_size) ** 2\n","        # Linear projection of flattened patches\n","        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n","        # Learnable classification token\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n","        # Learnable position embeddings\n","        self.pos_embed = nn.Parameter(torch.randn(1, self.n_patches + 1, embed_dim))\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, channels, height, width)\n","        batch_size = x.shape[0]\n","        # Project and flatten patches\n","        x = self.proj(x)  # (batch_size, embed_dim, h', w')\n","        x = x.flatten(2)  # (batch_size, embed_dim, n_patches)\n","        x = x.transpose(1, 2)  # (batch_size, n_patches, embed_dim)\n","\n","        # Add classification token\n","        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n","        x = torch.cat((cls_tokens, x), dim=1)\n","\n","        # Add position embeddings\n","        x = x + self.pos_embed\n","        return x\n","\n","class TransformerEncoder(nn.Module):\n","    def __init__(self, embed_dim=256, num_heads=8, mlp_ratio=4, drop_rate=0.1):\n","        super().__init__()\n","        # Multi-head Self-attention\n","        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=drop_rate, batch_first=True)\n","        # MLP block\n","        self.mlp = nn.Sequential(\n","            nn.Linear(embed_dim, mlp_ratio * embed_dim),\n","            nn.GELU(),\n","            nn.Dropout(drop_rate),\n","            nn.Linear(mlp_ratio * embed_dim, embed_dim),\n","            nn.Dropout(drop_rate)\n","        )\n","        # Layer normalization\n","        self.norm1 = nn.LayerNorm(embed_dim)\n","        self.norm2 = nn.LayerNorm(embed_dim)\n","\n","    def forward(self, x):\n","        # Attention block with residual connection\n","        x = x + self.attention(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n","        # MLP block with residual connection\n","        x = x + self.mlp(self.norm2(x))\n","        return x\n","\n","class VisionTransformer(nn.Module):\n","    def __init__(self, img_size=32, patch_size=4, in_channels=3,\n","                 embed_dim=256, depth=6, num_heads=8, mlp_ratio=4,\n","                 num_classes=10, drop_rate=0.1):\n","        super().__init__()\n","\n","        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n","        self.transformer_blocks = nn.ModuleList([\n","            TransformerEncoder(embed_dim, num_heads, mlp_ratio, drop_rate)\n","            for _ in range(depth)\n","        ])\n","        self.norm = nn.LayerNorm(embed_dim)\n","        self.head = nn.Linear(embed_dim, num_classes)\n","\n","    def forward(self, x):\n","        # Patch embedding\n","        x = self.patch_embed(x)\n","\n","        # Transformer blocks\n","        for block in self.transformer_blocks:\n","            x = block(x)\n","\n","        # Classification head\n","        x = self.norm(x)\n","        x = x[:, 0]  # Use only the [CLS] token\n","        x = self.head(x)\n","        return x\n","\n","# Training setup\n","def get_data_loaders(batch_size=128, train_subset_size=400, test_subset_size=100):\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ])\n","\n","    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                          download=True, transform=transform)\n","\n","    # Create a subset of the set\n","    train_subset = Subset(trainset, torch.arange(train_subset_size))\n","\n","    trainloader = DataLoader(train_subset, batch_size=batch_size,\n","                           shuffle=True, num_workers=2)\n","\n","    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                         download=True, transform=transform)\n","    test_subset = Subset(testset, torch.arange(test_subset_size))\n","    testloader = DataLoader(test_subset, batch_size=batch_size,\n","                          shuffle=False, num_workers=2)\n","\n","    return trainloader, testloader\n","\n","# Training loop\n","def train_model(model, trainloader, epochs=10, device='cpu'):\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.1)\n","\n","    model = model.to(device)\n","\n","    for epoch in tqdm(range(epochs), desc='Epochs'):\n","        model.train()\n","        running_loss = 0.0\n","        for i, (inputs, labels) in enumerate(trainloader):\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            if i % 100 == 99:\n","                print(f'[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}')\n","                running_loss = 0.0\n","\n","def evaluate_model(model, testloader, device='cpu'):\n","    model.eval()\n","    correct_1 = 0\n","    correct_5 = 0\n","    total = 0\n","    loss_total = 0\n","    criterion = nn.CrossEntropyLoss()\n","\n","    with torch.no_grad():\n","        for images, labels in testloader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss_total += loss.item()\n","\n","            # Top-1 accuracy\n","            _, predicted = outputs.max(1)\n","            correct_1 += predicted.eq(labels).sum().item()\n","\n","            # Top-5 accuracy\n","            _, top5_predicted = outputs.topk(5, 1)\n","            for i in range(labels.size(0)):\n","                if labels[i] in top5_predicted[i]:\n","                    correct_5 += 1\n","\n","            total += labels.size(0)\n","\n","    top1_accuracy = 100.0 * correct_1 / total\n","    top5_accuracy = 100.0 * correct_5 / total\n","    avg_loss = loss_total / len(testloader)\n","\n","    print(f'Top-1 Accuracy: {top1_accuracy:.2f}%')\n","    print(f'Top-5 Accuracy: {top5_accuracy:.2f}%')\n","    print(f'Average Loss: {avg_loss:.4f}')\n","\n","    return {\n","        'top1_accuracy': top1_accuracy,\n","        'top5_accuracy': top5_accuracy,\n","        'loss': avg_loss\n","    }\n","\n","\n","# Usage example\n","if __name__ == '__main__':\n","    device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n","\n","    # Initialize model with smaller parameters for MacBook Pro\n","    model = VisionTransformer(\n","        img_size=32,        # CIFAR-10 image size\n","        patch_size=4,       # 4x4 patches\n","        in_channels=3,      # RGB images\n","        embed_dim=256,      # Smaller embedding dimension\n","        depth=6,            # Fewer transformer layers\n","        num_heads=8,        # Number of attention heads\n","        num_classes=10      # CIFAR-10 classes\n","    )\n","    print('Initialized model')\n","    trainloader, testloader = get_data_loaders(batch_size=32)\n","    print('Loaded data')\n","    train_model(model, trainloader, epochs=1, device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hikHx2hL9l1w"},"outputs":[],"source":["# save model\n","torch.save(model.state_dict(), 'vit_cifar10.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gxKgjTDw9l1y","outputId":"aeebbfbf-d78a-4c20-8e97-228643aab2c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Top-1 Accuracy: 19.00%\n","Top-5 Accuracy: 69.00%\n","Average Loss: 2.1644\n"]},{"data":{"text/plain":["{'top1_accuracy': 19.0, 'top5_accuracy': 69.0, 'loss': 2.164409816265106}"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["metrics = evaluate_model(model, testloader, device)\n","metrics"]}],"metadata":{"kernelspec":{"display_name":"kaggle","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}